{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 6 : Clustering\n",
    "- Name:Eli Billinger\n",
    "- Date:10/4/2020 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "Practice clustering on a using the well known and very popular `Iris` Dataset! The Iris flower data set is fun for learning supervised classification algorithms, and is known as a difficult case for unsupervised learning. \n",
    "https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html\n",
    "<br><br>Yes, there are many examples out there, but see if you can do it yourself :). We can easily hypothesize on how many clusters would yield the best result, so let us prove it through a simple experiment that you could repeat with additional data sets.\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Data Generation (5 points)\n",
    "Reference for more information: Chapter 5.11 K-Means in the online course book.\n",
    "\n",
    "1. Load the `iris` dataset and separate into `X` and `y` variables (our ground truth labels will just be used for visualization).\n",
    "2. Write a hypothesis on how many clusters will yield the best labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**\n",
    ">\n",
    ">The best labeling will occur with 4 clusters becuase there are 4 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Data exploration (10 points)\n",
    "\n",
    "This is the step where you would normally conduct any needed preprocessing, data wrangling, and investigation of the data.\n",
    "<br>**Note:** `print(iris.DESCR)` prints the iris dataset description, provided you loaded it into a variable named `iris`\n",
    "\n",
    "a. Using your skills from previous projects, provide code below to produce answers to the following questions (edit this cell with your answers): \n",
    "\n",
    "    1. How many features are provided?\n",
    "\n",
    "    There are 4 features.\n",
    "\n",
    "    2. How many total observations?\n",
    "    \n",
    "    There are 150 observations.\n",
    "\n",
    "    3. How many different labels are included, what are they called, and is it a balanced dataset with the same number of observations for each class?\n",
    "        There are three different labels called Iris-Setosa,Iris-Versicolour,Iris-Virginica and each of the classe have 50 observations each.\n",
    "    \n",
    "        \n",
    "b. Create a 2D or 3D scatter plot of two or three of the features and use the y labels for color coding. Do not reduce the data or number of features in any way (you will do this by applying PCA in problem 5).\n",
    "\n",
    "c. Since clusters can be influenced by the magnitudes of the variables, normalize the feature data and plot a histogram of the normalized features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.scatter(X[:,0],X[:,1],y)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Normalization\n",
    "transformed = Normalizer().transform(X)\n",
    "plt.hist(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Unsupervised Learning - Clustering (15 points)\n",
    "Conduct clustering experiments with one of algorithms discussed in class (e.g., k-means) for number of clusters k = 2-10. Create another 2D or 3D scatter plot utilizing the <b>cluster assignments</b> for color coding (this output can be a plot for each of the values of k or just one final plot using the value of k from your best Silhouette result obtained in Problem 4 below).  \n",
    "\n",
    "#### Steps:\n",
    "Repeat for each value of k (maybe a loop here would be appropriate):\n",
    "1. Create model object\n",
    "2. Train or fit the model\n",
    "3. Predict cluster assignments\n",
    "4. Calculate Silhouette width (see Problem 4)\n",
    "4. Plot points color coded by class labels predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "silhouette = []\n",
    "for k in range(2,11):\n",
    "    model = KMeans(k)\n",
    "    model.fit(X,y)\n",
    "    prediction = model.predict(X,y)\n",
    "    silhouette.append(sk.metrics.silhouette_score(X,prediction))\n",
    "    plt.scatter(X[:,0],X[:,1],c=prediction)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Evaluate results (20 points)\n",
    "\n",
    "As we have discussed, validating an usupervised problem is difficult. There is a metric that can be used to determine the density or separation of cluster assignments, called Silhouette width. In this step, perform analysis of results using the above `k = 2-10` and compute the Silhouette width (Hint: possibly you can just add code to your loop in problem 3 and store the results in a list of values). \n",
    "\n",
    "Scikit Learn has a great example for Silhouette analysis [here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "1. For each k (k = 2-10), what are the Silhouette width values?\n",
    "\n",
    "k=2: 0.681046169211746\n",
    "k=3: 0.5528190123564091\n",
    "k=4: 0.4980505049972867\n",
    "k=5: 0.4887488870931048\n",
    "k=6: 0.3678464984712235\n",
    "k=7: 0.3571558397875374\n",
    "k=8: 0.3471194328049025\n",
    "k=9: 0.3455246689352856\n",
    "k=10: 0.3258225826755834\n",
    "\n",
    "2. Discuss if your best number of clusters (highest Silhouette width value) matches your hypothesis from Problem 1.\n",
    "\n",
    "The best number of clusters is 2 based on the silhouette values and that does not match my hypothesis. When looking at all the scatter plots of the clusters 2 clusters makes sense because the more cluster that are added the closer together they become and they start to have points overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in silhouette:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 (15 points): Principal Component Analysis (PCA)\n",
    "PCA is the most popular form of dimensionality reduction, which basically, rotates and transforms the data into a new subspace, such that the resultant matrix has:\n",
    "- Most relevance (variation) now associated with first feature\n",
    "- Second feature gets the next most, etc.\n",
    "#### Steps:\n",
    "1. Reduce the feature data (X) using PCA\n",
    "2. Repeat the same experiment from problem 3 above (remember your plots are now the 1st, 2nd, and possibly 3rd principal component vs. the raw feature data like before).\n",
    "3. Compare and contrast results to those from previous/non-PCA problems; does it perform better/worse/same? Provide discussion below (this could vary, depending on setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clustering with PCA\n",
    "silhouettePCA = []\n",
    "\n",
    "for k in range(2,5):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X)\n",
    "    xPCA = pca.transform(X)\n",
    "    model = KMeans(k)\n",
    "    model.fit(xPCA,y)\n",
    "    prediction = model.predict(xPCA,y)\n",
    "    silhouettePCA.append(sk.metrics.silhouette_score(X,prediction))\n",
    "    plt.scatter(xPCA[:,0],xPCA[:,1],c=prediction)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in silhouettePCA:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss new results**(Edit this cell)\n",
    ">\n",
    ">The silhouette values of the PCA and non PCA solutions are almost identifal to each other. The visual scatter plots are different but still give the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Finished! Treat yourself by taking this questionnaire\n",
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br><br>\n",
    "I spent about 5 hours on this assignment\n",
    "\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br><br>\n",
    "I liked how it demonstrated the differnt number of cluster in problem 3.\n",
    "\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br><br>\n",
    "No errors and change nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
