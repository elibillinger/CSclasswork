{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "In this project you will be learning about Convolutional Neural Networks (CNN) and messing with the parameters to see how they can change the output of the CNN. You will start with some questions about Tensorflow, the python package used for CNN! Good luck!\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 30 points\n",
    "\n",
    "### Instructions\n",
    "- The **\"MNIST\" data set** is composed of 28x28 pixel, black and white images of handwritten digits (0-9). It is commonly used to demonstrate the training and testing of CNNs.\n",
    "- **We provide you with framework code** to build a CNN of roughly suitable size for the MNIST digit recognition task.\n",
    "\n",
    "\n",
    "- **Your tasks**:\n",
    "    1. Read the linked article and answer the questions under the \"What is Tensorflow\" section\n",
    "    2. **Execute the train/test code** and observe the results of the gradient descent training.\n",
    "    3. Complete all 4 modifications of the code, thee modification instructions are:\n",
    "         - **Modify the model architecture in the following ways**, and repeat the training and testing. Each modification should be made relative to the original network. Do not keep \"adding\" each modification with each new model (which would lead to a final model having all the modifications below):\n",
    "           - **Modification 1**: Remove the ReLU activation functions and the max-pooling layers, thereby making the entire network a linear function.\n",
    "           - **Modification 2**: Increase the size of the model by a factor of 16, roughly.\n",
    "           - **Modification 3**: Convert the large convolutional model to a large non-convolutional model, with roughly the same number of parameters.\n",
    "           - **Modification 4**: Train the original baseline model with a larger training data set. \n",
    "     4. MAKE SURE THAT: along the way, **answer the questions in the Jupyter notebook cells**. You can just do this with comments within the cell, for example...\n",
    "   - ```## Answer 1: The training loss increased when I did x, y, and z.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "## What is TensorFlow? (6 pts)\n",
    "\n",
    "Before we get started with the rest of the project, let's talk about what tensorflow is. You are required to read this article https://www.guru99.com/what-is-tensorflow.html, and answer the following questions:\n",
    "\n",
    "1. What are the three parts of the TensorFlow Architecture?\n",
    "\n",
    "The three parts of the TensorFlow Architecture are preprocssing the data, build the model, and train and estimate the model.\n",
    "\n",
    "2. What is a Tensor and how does it represent data?\n",
    "\n",
    "A Tensor is a vector or matrix that holds a type of data.\n",
    "\n",
    "3. What is one advantage of the use of graphs in TensorFlow?\n",
    "\n",
    "One advantage of the use of graphs is that the portabillity of the grpah allows computations to be preserved for later use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Set TensorFlow verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 2 # 0==no output, 1=accuracy/loss output, 2=progress bar output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper function to plot results of our model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    epoch_num = np.arange(1, len(history.history['loss'])+1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_num, history.history['loss'], label='training_loss')\n",
    "    plt.plot(epoch_num, history.history['val_loss'], label='test_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_num, history.history['accuracy'], label='training_accuracy')\n",
    "    plt.plot(epoch_num, history.history['val_accuracy'], label='test_accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST data\n",
    "\n",
    "### You will not need to re-run or re-use (cut/paste) the three code cells below.\n",
    "\n",
    "The full dataset has 60,000 training images and 10,000 test images.\n",
    "\n",
    "To accelerate training on jupyterhub (with lots of users) we will only  \n",
    "use a subset of the training set. This will also let us explore some of the  \n",
    "hazards of training a neural net without a very large set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 1000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 1000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll show a few of the MNIST digits, to confirm that they look as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You will not need to run this cell more than once, or cut/paste it elsewhere\n",
    "plt.figure(figsize=(8*2, 2*2))\n",
    "for i in range(16):\n",
    "    plt.subplot(2, 8, i+1)\n",
    "    plt.imshow(images_train[i,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255\n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct, compile, and train the baseline model.\n",
    "\n",
    "### Don't edit any code in the cell below.\n",
    "\n",
    "### Run the cell, and wait for the training to finish. Training may take minute or two.\n",
    "\n",
    "The code will plot the loss and accuracy scores that were collected during training. Remember that training uses gradient descent, so the model parameters are slowly updated as the model gets a closer and closer fit to the data. TensorFlow records the scores after each \"epoch\"--the number of iterations at which all samples in the training set have been used once, in the gradient descent process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This is the baseline model. Only modify it after copying it to cells further below.\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFICATION 1 (6 pts)\n",
    "\n",
    "### Copy code from cell above, that builds and trains the baseline model, and plots the results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "Alter the model by removing the ReLU and max-pooling non-linear activations.  \n",
    "You can do this by\n",
    "1. setting ```activation=None``` rather than ```activation='relu'``` in the relevant NN layers, and\n",
    "2. by commenting out or deleting lines of code that define the max-pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 1\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "#%tensorboard --logdir logs\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation=None),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation=None),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation=None),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICATION 1\n",
    "## QUESTION 1: During training, how did the loss (error) curves change from the baseline model to the linear (MOD 1)\n",
    "##             model, for both the training and test sets? What does this imply regarding underfitting or overfitting?\n",
    "## QUESTION 2: In general, did the ReLU and max-pooling non-linearities make for a better model or a worse model?\n",
    "\n",
    "## Answer 1: From the basline model to the linear model the test line on the loss grpah continues to grow instead of \n",
    "##           going to zero like the basline model. This implies that the model is overfitting since it diverges.\n",
    "##\n",
    "## Answer 2: The RelU and mox-polling made for a better model.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFICATION 2 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "Make the network much larger, by:\n",
    "1. Increasing the number of kernels in the convolutional layers from 4 to 64 (16x).\n",
    "2. Increasing the number neurons in the dense layer from 64 to 1024 (16x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 2\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 64\n",
    "dense_layer_neurons = 1024\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICATION 2\n",
    "## QUESTION 1: How did the performance of the larger model (MOD 2) compare to that of the baseline model?\n",
    "## QUESTION 2: Based on the training curves, does the larger model show any **clear** signs of overfitting,\n",
    "##             despite the large number of parameters?\n",
    "\n",
    "## Answer 1: The preformance of the larger model was slightly worse than the baseline model since the test curves is still seperate from the training curves.\n",
    "##\n",
    "## Answer 2:The larger model does not show any clear signs of overfitting as the lines are parrallel and dont diverage.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFICATION 3 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "**Relative to the MOD 2 model**, remove the convolutional and max-pooling layers, replacing them with a single, new, dense layer.\n",
    "\n",
    "The two convolutional layers have 640 and 36,928 parameters, respectively, for a total of 37,586.\n",
    "For an input of 28x28 = 784 pixels (features), a dense layer with 48 neurons will have roughly 784x48 = 37,632 parameters. Thus, do the following:\n",
    "1. Remove the convolutional and max-pooling layers.\n",
    "2. After the Flatten() layer, add a new Dense layer with 48 neurons and a ReLU activation function.\n",
    "3. Keep the (now second) Dense layer of dense_layer_neurons==1024 neurons, and the final Dense layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MODIFICATION 3\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 64\n",
    "dense_layer_neurons = 1024\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "   # tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    #tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    #tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(48, activation='relu'),\n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICATION 3\n",
    "## QUESTION 1: How did the performance of the large non-convolutional model (MOD 3) compare to that of the\n",
    "##             large convolutional model (MOD 2)?\n",
    "## QUESTION 2: Speculate on why the convolutional model performs better or worse than the non-convolutional model.\n",
    "##             There is a \"right\" answer, but we're just looking for your opinion/guess. No penalty for error.\n",
    "\n",
    "## Answer 1: The large non-convolutional model performed worse than the large convolutinal model espcially for the\n",
    "##           loss grpah.\n",
    "## Answer 2: The convolutional model performs better because the more parameters in the dense allow for more loss to happen.\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIFICATION 4 (6 pts)\n",
    "\n",
    "## This model/data with take longer to train the the previous ones--upwards of several minutes.\n",
    "\n",
    "### For the modification, we don't modify the model, we modify the size of the data sets. The two cells below create training and testing sets with 10,000 samples each, whereas our previous models used only 1,000 samples each.\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "- Place the copied code after the two cells below.  \n",
    "- Run all the cells below to get the new data sets, and train the baseline model on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 10000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 10000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255\n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICATION 4\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFICATION 4\n",
    "## QUESTION 1: How did the performance of the baseline model trained on the larger data set (MOD 4) compare\n",
    "##             to that trained on the smaller data set?\n",
    "## QUESTION 2: This is just guess on your part... how much better do you think results might be if you trained\n",
    "##             the model on all 60,000 training samples (rather than 1000 or 10,000)?\n",
    "\n",
    "## Answer 1: The smaller set performed better than on the larger data set.\n",
    "##\n",
    "## Answer 2: If it was trained on all 60,000 samples I think that it would perform better since instead of a sample\n",
    "##           of data being used that could be inconsistent for the entire dataset the whole dataset would be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br><br>\n",
    "I spent about 2 hours on this assignment.\n",
    "\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br><br>\n",
    "I liked how the assignment had us change different parts to see how it affected the model.\n",
    "\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br><br>\n",
    "No errors and change nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
